{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab5-shazam.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 120 Lab 8: Shazam\n",
    "\n",
    "**Signals and Systems** at UC Berkeley\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- **Spring 2020** (v1.0): Anmol Parande, Dominic Carrano, Babak Ayazifar\n",
    "- **Spring 2022** (v2.0): Anmol Parande\n",
    "- **Spring 2023** (v2.1): Yousef Helal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In 2002, Shazam Entertainment Limited (founded by UC Berkeley students!) launched its music identification product, allowing users to dial a phone number and play a song. Then, they'd get a text message with the name of the song and its artist. In 2018, Shazam was acquired by Apple for \\$400 million, and it's now in every iPhone.\n",
    "\n",
    "Shazam works by using *audio fingerprinting*: given a song, it generates a set of identifiers, and searches an audio database to find a match and identify the song. In this lab, you'll learn about audio fingerprinting, and use it to build a music identification just like Shazam!\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To get started, you'll need to install [Pandas](https://pandas.pydata.org/docs/getting_started/install.html). And, of course, you'll also need NumPy, SciPy, and Matplotlib if you don't already have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.ndimage import maximum_filter\n",
    "from shazam_utils import generate_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To avoid any copyright issues, we've cropped all provided songs to only contain the first 60 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Spectral Analysis\n",
    "\n",
    "As we've already seen throughout the course, a signal's constituent frequencies tell us a lot about it. The same is true of audio: to find the salient features of songs to fingerprint, we'll need to look at the song's spectrum (i.e., Fourier Transform). Fortunately, we have the DFT (efficiently implemented via the FFT) to help us do this.\n",
    "\n",
    "To get started, let's load in *Viva La Vida* by Coldplay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we think of audio as a two-channel, continuous signal $\\vec{x}(t) = \\left[x_L(t) \\ x_R(t)\\right]$, with one column of the *audio matrix* per channel. That is, $x_L(t)$ is the left channel's signal, and $x_R(t)$ the right channel's signal. The reason we have two distinct audio channels is so that we can have two streams playing at the same time, one per ear (e.g., in a pair of headphones or laptop speakers).\n",
    "\n",
    "We sample this CT audio signal at a particular rate (here, 48000 Hz) to get a DT signal. For our purposes, the distinction between our channels is not very important, so we'll just average them to form a 1D signal, $x(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "print(\"Audio Shape: {0}, Sampling Rate: {1} Hz\".format(coldplay.shape, fs))\n",
    "coldplay = np.mean(coldplay, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense for the song we're working with, feel free to have a listen! This cell may take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"VivaLaVida.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1a: One DFT is Not Enough\n",
    "\n",
    "As far as spectral analysis is concerned, it seems like we should just be able to take the DFT of the entire song, find our fingerprints, and be done, right? Is that really all there is to Shazam? No, not quite. It may not be obvious, but there's a big issue with this approach that we'll explore now. So that our code doesn't take forever to run, we'll only look at the first 10 seconds of the song, but the issues we'll find here apply generally to the entire signal.\n",
    "\n",
    "To start, lets define a function which will give us the magnitude spectrum of the signal $|X(\\omega)|$ centered around $\\omega = 0$. By default, when you compute the FFT, the samples of the DTFT that are returned go from $0$ to $2\\pi$; Centering them so that they go from $-\\pi$ to $\\pi$ is nicer for visualization.\n",
    "\n",
    "Fill in the code for `centered_magnitude`, which takes in a signal and outputs its centered magnitude spectrum.\n",
    "\n",
    "**Hint**: Check out [np.fft.fftshift](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftshift.html) to center your spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def centered_magnitude_spectrum(sig):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why one DFT won't suffice, we're going to look at the spectrum of different sections of Viva La Vida.\n",
    "\n",
    "First, we'll look at magnitude spectrum of the first 10 seconds of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldplay_cropped = coldplay[:10 * fs]\n",
    "coldplay_freqs = centered_magnitude_spectrum(coldplay_cropped)\n",
    "\n",
    "plt.figure(figsize=(16, 4), dpi=200)\n",
    "freqs = np.linspace(-fs/2, fs/2, len(coldplay_freqs))\n",
    "plt.plot(freqs, coldplay_freqs)\n",
    "plt.xlabel(\"Frequency [Hz]\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"DFT of first 10 seconds of Viva La Vida\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the frequency content is centered around the lower frequencies. In fact, we can barely see anything past 10 kHz, because human hearing stops around 15-20 kHz (and generally decreases with age), so there's no reason to include anything that high in music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks ok: we got the spectrum of the first 10 seconds of our song. This gives us a sort of \"aggregate view\" of the frequencies that show up at some point during the first 10 seconds. But is this \"aggregate view\" good enough? What happens if our signal is *non-stationary*, i.e its frequency content changes with time, as is certainly the case with music? \n",
    "\n",
    "To find out, lets look at the magnitude spectra of the first, second, third, and fourth seconds of the song. We'll use these to zoom in (temporally speaking) and inspect the song's frequency content over the course of a second of data (rather than 10), and see if the \"aggregate view\" gives a good enough picture of what frequencies are present at a specific second in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldplay_freqs_1 = centered_magnitude_spectrum(coldplay[:fs]) \n",
    "coldplay_freqs_2 = centered_magnitude_spectrum(coldplay[fs:2*fs])\n",
    "coldplay_freqs_3 = centered_magnitude_spectrum(coldplay[2*fs:3*fs])\n",
    "coldplay_freqs_4 = centered_magnitude_spectrum(coldplay[3*fs:4*fs])\n",
    "\n",
    "freqs = np.linspace(-fs / 2, fs / 2, len(coldplay_freqs_1))\n",
    "sigs = [coldplay_freqs_1, coldplay_freqs_2, coldplay_freqs_3, coldplay_freqs_4]\n",
    "strs = [\"1st\", \"2nd\", \"3rd\", \"4th\"]\n",
    "\n",
    "plt.figure(figsize=(16, 10), dpi=200)\n",
    "for i in range(1, 5):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(freqs, sigs[i-1])\n",
    "    plt.xlim([-.5e4, .5e4])\n",
    "    plt.ylim([0, 1.1 * np.array(sigs).max()])\n",
    "    plt.title(\"DFT magnitude of {} second of Viva La Vida\".format(strs[i-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how while most of the energy in each second's spectrum is concentrated inside $[-2.5 \\text{ kHz}, +2.5 \\text{ kHz}]$, the exact shapes are quite different. \n",
    "\n",
    "**The issue is that the aggregate view from our 10-second DFT doesn't have good enough *temporal resolution*: we can't see how the signal's frequency content changes over time!**\n",
    "\n",
    "Why does this matter, you ask? Well, when we're working with the real deal, we don't feed Shazam the entire song; only a clip. For example, suppose you tune into a radio station halfway through a song. Then, 20 seconds later, you think to yourself, \"hey, I like this\" and pull out Shazam to figure out what song it is. By then, whatever you're giving Shazam is missing a lot of data, and so it needs to be able to look at what frequencies are in the song at different points in time to correctly identify it. The aggregate view won't do. Fortunately, there's a very simple fix to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1b: Spectrogrammin'\n",
    "\n",
    "The results of Q1a are pretty clear: we need a way to see how the signal's frequency content changes over time. Just taking one DFT of the entire signal fails to achieve this. Instead, we'll use a *spectrogram*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms\n",
    "\n",
    "A *spectrogram* is an image representing the frequency content of a signal at different times. This ability to see how a signal's frequency content changes with time is the key useful feature of a spectrogram. \n",
    "\n",
    "To compute a spectrogram, we split our signal into chunks, compute the DFT of each chunk, and plot the magnitude squared of those DFT chunks side-by-side. To make visualization easier, we typically employ a colormap to distinguish where the DFT's squared-magnitude is bigger.\n",
    "\n",
    "For example, here is a spectrogram of speech, taken from [here](https://www.researchgate.net/figure/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are_fig1_319081627). The red areas correspond to stronger frequency content, and green areas to weaker frequency content.\n",
    "\n",
    "Notice the differences between when the speaker takes a breath and when the speaker is actually speaking. A single DFT wouldn't be able to separate this!\n",
    "\n",
    "![image.png](speech-spectrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some familiarity with spectrograms, let's generate some sinusoidal signals and plot their spectrograms. You don't need to write any code for this question. Just run the cells!\n",
    "\n",
    "In the cell below, we generate 1000 samples of the following signals over the interval $t \\in [0, 1]$:\n",
    "- A 100 Hz sine wave (call it `x1`).\n",
    "- A 400 Hz sine wave (call it `x2`).\n",
    "- A third signal, call it `x3`, by concatenating `x1` and `x2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 1, 1000)\n",
    "x1 = np.sin(2 * np.pi * 100 * n)\n",
    "x2 = np.sin(2 * np.pi * 400 * n)\n",
    "x3 = np.concatenate((x1, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the DFT of our signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_1 = centered_magnitude_spectrum(x1)\n",
    "freqs_2 = centered_magnitude_spectrum(x2)\n",
    "freqs_3 = centered_magnitude_spectrum(x3)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n",
    "\n",
    "axs[0].plot(np.linspace(-500,500, len(freqs_1)), freqs_1)\n",
    "axs[0].set_ylabel('DFT Magnitude')\n",
    "axs[0].set_xlabel('Frequency [Hz]')\n",
    "axs[0].set_title(\"100 Hz Signal\")\n",
    "\n",
    "axs[1].plot(np.linspace(-500,500, len(freqs_2)), freqs_2)\n",
    "axs[1].set_ylabel('DFT Magnitude')\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].set_title(\"400 Hz Signal\")\n",
    "\n",
    "axs[2].plot(np.linspace(-500,500, len(freqs_3)), freqs_3)\n",
    "axs[2].set_ylabel('DFT Magnitude')\n",
    "axs[2].set_xlabel('Frequency [Hz]')\n",
    "axs[2].set_title(\"100 Hz and 400 Hz Signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the *pure tones* (the 100 Hz and 400 Hz sine waves) have 2 peaks each, due to conjugate symmetry, whereas the signal formed by concatenating them has 4 peaks. Now, let's look at the spectrograms of these signals. Run the following code to plot the spectrogram of each signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, t1, x1_freqs = signal.spectrogram(x1, fs=1000)\n",
    "f2, t2, x2_freqs = signal.spectrogram(x2, fs=1000)\n",
    "f3, t3, x3_freqs = signal.spectrogram(x3, fs=1000)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n",
    "\n",
    "axs[0].pcolormesh(t1, f1, x1_freqs, cmap=\"gray\", shading='auto')\n",
    "axs[0].set_ylabel('Frequency [Hz]')\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].set_title(\"100 Hz Signal\")\n",
    "\n",
    "axs[1].pcolormesh(t2, f2, x2_freqs, cmap=\"gray\", shading='auto')\n",
    "axs[1].set_ylabel('Frequency [Hz]')\n",
    "axs[1].set_xlabel('Time [sec]')\n",
    "axs[1].set_title(\"400 Hz Signal\")\n",
    "\n",
    "axs[2].pcolormesh(t3, f3, x3_freqs, cmap=\"gray\", shading='auto')\n",
    "axs[2].set_ylabel('Frequency [Hz]')\n",
    "axs[2].set_xlabel('Time [sec]')\n",
    "axs[2].set_title(\"100 Hz and 400 Hz Signal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first spectrogram has a single band at 100 Hz. The second has a single band at 400 Hz. The final one has two bands (one at 100 Hz and one at 400 Hz). The reason we aren't seeing conjugate symmetry here is because we are only plotting the positive frequencies. For the most part, these spectrograms appear to give us the same information as the DFT. \n",
    "\n",
    "However, notice that in the 3rd spectrogram, the frequencies are mostly only present for the duration they exist. There's some overlap between 1.0-1.2 seconds, which isn't what we would have expected. This happens because SciPy doesn't truly use distinct chunks, as we mentioned above, and instead goes with a more sophisticated overlapping window approach, covered in EE 123 (this gives a better tradeoff between the temporal and spectral resolutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1c: Spectrograms of Songs\n",
    "\n",
    "Now that we've got the basic concepts down, let's load *Viva La Vida* and *Mr. Brightside* and compare their spectrograms. Run the cell below to load the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "coldplay = np.mean(coldplay, axis=1)\n",
    "\n",
    "fs, killers = wavfile.read(\"MrBrightside.wav\")\n",
    "killers = np.mean(killers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't heard *Mr. Brightside* yet, let's load it in now and have a listen. This cell will take a few seconds to load before the audio interface shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"MrBrightside.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better looking image when visualizing the spectrogram, we'll plot everything in decibels. \n",
    "\n",
    "Recall that to convert a number $x$ to decibels, we compute $x_\\text{dB} = 20\\log_{10}(x).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Job\n",
    "\n",
    "In the cell below:\n",
    "1. Use [`signal.spectrogram`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html) to compute the spectrogram of each song. \n",
    "    - Use 4096 for the `nperseg` parameter of `signal.spectrogram` to take a 4096 point DFT. This matches the length of DFT typically used in practical audio fingerprinting systems, representing a good tradeoff between spectral and temporal resolution.\n",
    "    - The function returns a tuple containing the frequencies of the spectrogram samples, time points of the spectrogram samples, and the actual spectrogram. For each call to `signal.spectrogram`, store these as `fi, ti, []_spect` where `i` is 1 or 2 and `[]` gets filled with `coldplay` or `killers` depending on the song.\n",
    "2. Convert the resultant spectrograms to the decibel scale using the formula from above.\n",
    "\n",
    "Don't worry about any divide-by-zero issues that happen due to taking the log. If you're concerned, you can add a small positive constant (say, $10^{-12}$) as a remedy to silence the warning.\n",
    "\n",
    "*Hint*: make sure you use `np.log10` in your computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the spectrogram of Viva La Vida and Mr. Brightside in decibels\n",
    "\n",
    "coldplay_spect = ...\n",
    "killers_spect = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look! Run the cell below to plot the spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10), dpi=200)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.pcolormesh(t1, f1, coldplay_spect, cmap=\"jet\", shading=\"auto\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "plt.xlabel(\"Time [sec]\")\n",
    "plt.title(\"Viva La Vida\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(t2, f2, killers_spect, cmap=\"jet\", shading=\"auto\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "plt.xlabel(\"Time [sec]\")\n",
    "plt.title(\"Mr. Brightside\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Based on the spectrograms above, answer the following questions:\n",
    "\n",
    "**Q:** In both spectrograms, we see a column of dark blue for the first second or so. Based on our colorbar, it looks like this corresponds to $\\approx -300 \\text{dB}$, or essentially no signal power. In terms of the songs, why do we have this in our plots?\n",
    "\n",
    "**Q:** At the beginning of the spectrogram for Mr. Brightside (after the column of dark blue), you should see two peaks that extend up toward $20 \\text{ kHz}$. What sound in the song is this part of the spectrogram capturing?\n",
    "\n",
    "**Q:** Can you easily tell the two songs' spectrograms apart? Do you think they'd make good building blocks for our audio recognition algorithm?\n",
    "</hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<span style=\"color:blue\">**A:** (TODO) </span>\n",
    "\n",
    "<span style=\"color:blue\">**A:** (TODO) </span>\n",
    "\n",
    "<span style=\"color:blue\">**A:** (TODO) </span>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Fingerprinting\n",
    "\n",
    "Our end goal here is to take an audio snippet and figure out what song's being played. To do this, we'll need a database of songs to compare against. \n",
    "\n",
    "Should we just store entire songs in the database? Probably not, as that'd be a very large database: a three-minute WAV file sampled at $48 \\text{ kHz}$ is a about $30 \\text{ MB}$ in size. Even if we aimed for the modest goal of 1000 songs (which the original iPod from 2001 could hold), we're already looking at using over $30 \\text{ GB}$ of storage. Additionally, comparing raw audio samples for similarity isn't very robust against noise.\n",
    "\n",
    "Instead, we'll generate a set of *fingerprints* from each song, and store these in our database. When our version of Shazam gets fed a song to classify, it can just compare the fingerprints, rather than looking at the whole song. This should solve our storage issues, provided the fingerprints aren't too large. But clearly we'll need this fingerprinting algorithm to have a few other properties for this audio recognition system to be useful.\n",
    "\n",
    "In particular, we want our audio fingerprint to have four key properties:\n",
    "1. ***Temporal Locality:*** We're trying to figure out what song is being played based on a short (say, 5 to 10 second long) clip. So, our fingerprints should somehow encode *where* in the song they come from.\n",
    "\n",
    "2. ***Translational Invariance:*** The snippet we play for Shazam could come from anywhere in the song. We could play it the first 5 seconds, the last 5, or something in the middle. In all cases, we want a correct result, so the same chunk of audio should get the same fingerpint regardless of whether it shows up a minute into a clip or right at the beginning—it's the actual music in it that we should use to generate the fingerprint.\n",
    "\n",
    "3. ***Robustness:*** An audio file, whether clean or degraded by (a modest amount of) noise, should produce the same fingerprint.\n",
    "\n",
    "4. ***High Entropy:*** The fingerprinting algorithm should be \"random enough\" that two different songs don't produce the same fingerprint.\n",
    "\n",
    "As it turns out, spectrograms have all these nice properties, which is why they're such an important part of Shazam! The company's founders recognized this too, and discussed it in their original paper, linked in the references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, spectrograms are cool, but how can we use them? They contain thousands of points... how do we pick which are the most important?**\n",
    "\n",
    "As you might guess, we'll look at the spectrogram's *peaks*: points in high-energy areas. These are the most likely to survive distortions from noise, unlike ones that are close to zero and easily drowned out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a: Peak Finding\n",
    "\n",
    "To extract these peaks, we want to find areas of the spectrogram where's there's some point that has more energy than its neighbors. To do this, we're going to need some non-linear filtering. \n",
    "\n",
    "### Max Filtering \n",
    "\n",
    "So far in this course, we have almost entirely worked with LTI systems as they're amenable to analysis. However, LTI filtering can't help us here because a \"maximum\" operation is fundamentally non-linear, failing to satisfy the superposition property. \n",
    "\n",
    "To do our peak finding, we'll use Scipy's [`maximum_filter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.maximum_filter.html) function with a neighborhood of 51 (the `size` parameter in the function call). \n",
    "\n",
    "For each point in our spectrogram, this filter will take our spectrogram $f(x, y)$ and output $g(x, y)$, the maximum value in a 51x51 region around the pixel. \n",
    "\n",
    "Formally,\n",
    "\n",
    "$$g(x, y) = \\max_{i,j} f(x+i, y+j) \\text{  where } -25\\le i, j \\le 25.$$\n",
    "\n",
    "### Your Job\n",
    "\n",
    "Implement the maximum filter and apply it to the spectrogram of \"Viva La Vida.\" When the neighborhood exceeds the boundary of the image, assume $f(x, y)$ is the value of the image at that point (i.e., set `mode='constant'`).\n",
    "\n",
    "\n",
    "After applying the maximum filter:\n",
    "1. Extract a boolean mask which is True when $f(x, y) = g(x, y)$, and False otherwise.\n",
    "2. To ensure these peaks are big enough, in the mask, set any peak locations with a peak less than or equal to `AMP_THRESH` to zero. You can easily, but are not required to, accomplish this with a bitwise `&` operation.\n",
    "3. Use [`np.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) to convert your mask into a set of (frequency, time) pairs. This function will return two arrays. The first is the indices along the frequency axis of the spectrogram where the peaks show up, and the second is the peak indices along the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEIGHBORHOOD_SIZE = 2 * 25 + 1\n",
    "AMP_THRESH = 40\n",
    "\n",
    "# TODO: Apply a Maximum Filter\n",
    "max_spect = ...\n",
    "\n",
    "# TODO: Compute the mask\n",
    "mask = ...\n",
    "\n",
    "# TODO: Filter out tiny peaks\n",
    "...\n",
    "\n",
    "# TODO: Get the indices of the peaks\n",
    "freq_idx, time_idx = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a (code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the next cell and see where our peaks are! We'll label them with black dots for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6), dpi=200)\n",
    "plt.scatter(t1[time_idx], f1[freq_idx], zorder=99, color='k')\n",
    "plt.pcolormesh(t1, f1, coldplay_spect, zorder=0, cmap=\"jet\", shading=\"auto\")\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.title(\"Spectrogram Peaks (for Viva La Vida)\")\n",
    "plt.xlim([0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Q:** In Q1, we saw how most of the information in music signals is in the lower frequencies (under, say, $10 \\text{ kHz}$). How does this compare with the spectrogram peaks? Are they mostly in lower or upper half of the spectrgram? Is this what you'd expect?\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<span style=\"color:blue\">**A:** (TODO)</span>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2b: Hashing\n",
    "\n",
    "The peaks we've found make up what the creators of Shazam call a *constellation map*. We'll use the points in our constellation map to compute the song's fingerprints. \n",
    "\n",
    "To do this, we'll take each peak, say $(t_i, f_i)$, and chain it together with the next 15 peaks $(t_{i+1}, f_{i+1}), ..., (t_{i+15}, f_{i+15})$ by:\n",
    "1. Subtracting the times where the peaks show up, computing $t_d = t_{i+j} - t_i$.\n",
    "2. Hash the string $[f_i:f_{i+j}:t_d]$. The fingerprint is $(\\text{hash}, t_i)$.\n",
    "\n",
    "Hashing is out of the scope of this course, so we've provided the hashing function for you (`generate_hash`). It takes  arguments $f_i, f_{i+j}, t_i, t_{i+j}$ (in that order), and does steps 1 and 2, returning the fingerprint, $(\\text{hash}, t_i)$.\n",
    "\n",
    "We've provided `sorted_peaks`, a list of all the peaks sorted in increasing order by the time at which they occur in the song. Note that each element of `sorted_peaks` contains two elements, the $(f, t)$ tuple indicating where the peak occured on the spectrogram. That is, `sorted_peaks[i]` contains the tuple $(f_i, t_i)$.\n",
    "\n",
    "### Your Job\n",
    "\n",
    "Using this, your job is to, for each peak in `sorted_peaks`:\n",
    "1. Extract $(f_i, t_i)$.\n",
    "2. Iterate over the next `HASHES_PER_PEAK` peaks (if you've hit the end of the array, then stop chaining the current peak and move onto the next peak), and:\n",
    "    - Extract $(f_{i+j}, t_{i+j})$.\n",
    "    - Generate the hash using the `generate_hash` function with $f_i, f_{i+j}, t_i, t_{i+j}$.\n",
    "    - Append it to `hashes`.\n",
    "\n",
    "Remember, we only found the indices of the peaks, not the actual times and frequencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HASHES_PER_PEAK = 15\n",
    "times, freqs = t1[time_idx], f1[freq_idx]\n",
    "sorted_peaks = sorted(zip(freqs, times), key=lambda x: x[1])\n",
    "hashes = []\n",
    "\n",
    "print(len(times), len(freqs))\n",
    "\n",
    "# TODO populate \"hashes\" with the fingerprints\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2c: Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can easily compute the fingerprint of a signal! After fingerprinting, all we need to do is search our database for a match. If we did things correctly, the database entry we have the most fingerprints in common with should match the true song.\n",
    "\n",
    "Let's move all of this code into a single function so we can easily compute hashes for any audio signal.\n",
    "\n",
    "`fingerprint` should return an array of tuples, each one containing the hash $h$ and the time $t_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fingerprint(audio, fs, min_distance=25, amp_thresh=40, hashes_per_peak=15):\n",
    "    NEIGHBORHOOD_SIZE = 2 * min_distance + 1\n",
    "    AMP_THRESH = amp_thresh\n",
    "    HASHES_PER_PEAK = hashes_per_peak\n",
    "    \n",
    "    audio = np.mean(audio, axis=1)\n",
    "    \n",
    "    # TODO: Compute the spectrogram of the single channel audio (Copy from Q1c) \n",
    "    f1, t1, spect = ...\n",
    "\n",
    "    # TODO: Apply a Maximum Filter (Copy from Q2a)\n",
    "    max_spect = ...\n",
    "\n",
    "    # TODO: Compute the mask (Copy from Q2a)\n",
    "    mask = ...\n",
    "\n",
    "    # TODO: Filter out tiny peaks (Copy from Q2a)\n",
    "    ...\n",
    "\n",
    "    # TODO: Get the indices of the peaks (Copy from Q2a)\n",
    "    freq_idx, time_idx = ...\n",
    "    \n",
    "    # TODO: Compute the hashes (Copy from Q2b)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Testing\n",
    "As mentioned before, all we need to do now is test our system and make sure it's as robust as we think it is. Our database is stored in `database.csv`. It's columns are |Hash|t1|Song|. A production application with thousands of songs in the database would use SQL or some other querying language, but a simple CSV will suffice for our uses.\n",
    "\n",
    "Because searching through our database is more of a software problem than a Signals and Systems problem, we've provided the detection function for you. \n",
    "\n",
    "This function:\n",
    "1. Loads the CSV using pandas (a data analysis package),\n",
    "2. Fingerprints the unknown sample,\n",
    "3. Searches for matches, and\n",
    "4. Returns the song with the most matches, its confidence as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(audio, fs):\n",
    "    db = pd.read_csv(\"database.csv\", header=None, names=[\"Hash\", \"time\", \"Song\"])\n",
    "\n",
    "    hashes = fingerprint(audio, fs)\n",
    "    db_matches = db[db.Hash.isin(map(lambda x: x[0], hashes))]\n",
    "    if len(db_matches) == 0:\n",
    "        print(\"No Matches\")\n",
    "        return\n",
    "\n",
    "    counts = db_matches.groupby(\"Song\").size()\n",
    "    counts = counts / counts.sum()\n",
    "    return counts.idxmax(), counts.max() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3a: Basic Testing\n",
    "\n",
    "Let's see how our system does under ideal conditions (i.e, no noise). Take a 20 second segment from Viva La Vida and Mr. Brightside and call the detect function to identify it. We've already reloaded the audio for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "fs, killers = wavfile.read(\"MrBrightside.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Take an unknown segment from Viva La Vida and try to detect it\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Take an unknown segment from Mr. Brightside and try to detect it\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3b: Gaussian Noise\n",
    "We want our system to be robust to different forms of noise. To start with, lets add some Gaussian noise to our audio and try to detect its origin. Take a 20 second chunk of Viva La Vida, add Gaussian noise with a mean and variance of 10000, and see if you can identify them. \n",
    "\n",
    "*Hint 1*: Checkout the [`np.random.normal`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) function. \n",
    "<br>\n",
    "*Hint 2*: Make sure to pass the `size` parameter to `np.random.normal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment_gaussian.T, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment_gaussian.T, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3c: Blocked Speaker\n",
    "\n",
    "What if instead of Gaussian noise, a portion of the audio just becomes zero? Arguably, this is a more realistic model of how our signal could get corrupted when dealing with music recognition. For example, somebody could move in front of the speaker, pause the music, or turn the volume down very low. \n",
    "\n",
    "Let's take a 20 second chunk of Viva La Vida, zero out five random 2 second chunks, and see if we can still detect the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment.T, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detect(unknown_segment, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment.T, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detect(unknown_segment, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comments (Optional)\n",
    "There are many ways to improve our Shazam system. Many of them have to do with how we compute our spectrogram as well as the various parameters we introduced such as `NEIGHBORHOOD_SIZE`, `AMP_THRESH`, and `HASHES_PER_PEAK`. But, for the most part, this is how Shazam works!\n",
    "\n",
    "The original Shazam paper uses a different method for matching the fingerprints of audio instead of a simple \"most matches => song\" scheme, but for our limited database, this works just fine. Check out the original paper if you are curious. If you'd like, you can use the following cells to load your own songs into the database (as long as they are wav files) and try to identify samples of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def add_to_db(filename):\n",
    "    fs, audio = wavfile.read(filename)\n",
    "    hashes = fingerprint(audio, fs)\n",
    "    with open('database.csv', mode='a') as db_file:\n",
    "        db_writer = csv.writer(db_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        for hash_pair in hashes:\n",
    "            db_writer.writerow([hash_pair[0], hash_pair[1], filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to add a song to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wav_filepath = ___ # path to any WAV file you want to add to the database\n",
    "add_to_db(my_wav_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] *An industrial strength audio search algorithm.* [[Link](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)].  \n",
    "[2] *Audio fingerprinting with Python and Numpy.* [[Link](https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please upload the zip file produced by the result of this command to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(force_save=True, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.allclose(centered_magnitude_spectrum(np.ones(10)), np.array([ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.]))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.allclose(centered_magnitude_spectrum([10] * 5 + [6 * 2] + [3] * 4), np.array([ 2.        ,  9.        ,  5.46491887,  9.        , 21.6364198 ,\n...        74.        , 21.6364198 ,  9.        ,  5.46491887,  9.        ]))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> x = np.array([0.77498361, 0.02466649, 0.5159193 , 0.75489558, 0.85777587,\n...         0.56253859, 0.01691631, 0.65058933, 0.4358812 , 0.80630942])\n>>> assert np.allclose(centered_magnitude_spectrum(x), np.array([0.19752313, 1.15615072, 0.59527128, 1.55793533, 0.21334073,\n...         5.4004757 , 0.21334073, 1.55793533, 0.59527128, 1.15615072]))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a (code)": {
     "name": "q2a (code)",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.allclose(freq_idx[10:20], np.array([20, 22, 22, 24, 27, 40, 40, 45, 45, 53]))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.allclose(time_idx[30:40], np.array([132, 597, 784, 311, 421, 454, 357, 637, 271, 245]))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(hashes) == 2970\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert hashes[:5] == [('eb046cc61c9c72cbc3e9', 1.6853333333333333), ('c3732d9a4bc79b82286e', 1.6853333333333333), ('8ff87ca5f63e8a036f46', 1.6853333333333333), ('793fe791fbba01ae5dac', 1.6853333333333333), ('c466941e32a557ad2b59', 1.6853333333333333)]\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 30,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_fs, test_coldplay = wavfile.read(\"VivaLaVida.wav\")\n>>> \n>>> assert np.allclose([pr[1] for pr in fingerprint(test_coldplay, test_fs)[100:120]], [3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667])\n>>> assert np.allclose([pr[1] for pr in fingerprint(test_coldplay, test_fs, min_distance=10)[100:120]], [0.864, 0.864, 0.864, 0.864, 0.864, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334])\n>>> assert np.allclose([pr[1] for pr in fingerprint(test_coldplay, test_fs, amp_thresh=25)[100:120]], [3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667])\n>>> assert np.allclose([pr[1] for pr in fingerprint(test_coldplay, test_fs, hashes_per_peak=10)[100:120]], [5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 5.866666666666666, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667, 6.314666666666667])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert detect(killers[10 * fs: 30 * fs], fs) == ('MrBrightside.wav', 100.0)\n>>> assert detect(coldplay[10 * fs: 30 * fs], fs) == ('VivaLaVida.wav', 100.0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> unknown_segment = killers[10*fs:30*fs]\n>>> unknown_segment_gaussian = unknown_segment + np.random.normal(loc=2500, scale=2500, size=unknown_segment.shape)\n>>> assert detect(unknown_segment_gaussian, fs) == ('MrBrightside.wav', 100.0)\n>>> \n>>> unknown_segment = coldplay[10 * fs:30 * fs]\n>>> unknown_segment_gaussian = unknown_segment + np.random.normal(loc=2500, scale=2500, size=unknown_segment.shape)\n>>> assert detect(unknown_segment_gaussian, fs) == ('VivaLaVida.wav', 100.0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> unknown_segment = coldplay[10 * fs: 30 * fs].copy()\n>>> unknown_segment[:2 * fs] = 0\n>>> unknown_segment[6 * fs:8 * fs] = 0\n>>> unknown_segment[16 * fs:20 * fs] = 0\n>>> unknown_segment[2 * fs:4 * fs] = 0\n>>> \n>>> assert detect(unknown_segment, fs) == ('VivaLaVida.wav', 100.0)\n>>> \n>>> unknown_segment = killers[10 * fs: 30 * fs].copy()\n>>> unknown_segment[:2 * fs] = 0\n>>> unknown_segment[6 * fs:8 * fs] = 0\n>>> unknown_segment[16 * fs:20 * fs] = 0\n>>> unknown_segment[2 * fs:4 * fs] = 0\n>>> \n>>> assert detect(unknown_segment, fs) == ('MrBrightside.wav', 100.0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
