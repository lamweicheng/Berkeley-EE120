{"cells":[{"metadata":{"deletable":false,"editable":false,"trusted":false},"cell_type":"code","source":"# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab5-deconvolution.ipynb\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EE 120 Lab 5: Deconvolution and Imaging\n\n**Signals and Systems** at UC Berkeley\n\nAcknowledgements:\n\n- **Spring 2019** (v1.0): Dominic Carrano, Ilya Chugunov, Babak Ayazifar  \n- **Fall 2019** (v2.0): Dominic Carrano, Ilya Chugunov  \n- **Spring 2020** (v2.1): Dominic Carrano, Ilya Chugunov\n- **Spring 2022** (v3.0): Anmol Parande"},{"metadata":{"trusted":false},"cell_type":"code","source":"import IPython.display as ipd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.signal\nfrom scipy import signal, linalg\nfrom scipy.io import wavfile\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Background\n\nNote: For simplicity, we'll consider all signals and systems in this lab as discrete-time entities, since that's what computers use. In truth, there are other steps involved, such as sampling and quantization."},{"metadata":{},"cell_type":"markdown","source":"### Motivation: Signal Restoration\n\n*Degradation* (also referred to as *distortion*) is a common problem in signal and image processing: you want access to a \"ground truth\" signal $x(n)$, but due to some real-world physical constraints, you can only observe $y(n)$, a corrupted version of $x(n)$. \n\nFor example, suppose you're playing a song $x(n)$ in a large concert hall. Due to the acoustics of the room, your audience hears $y(n)$, the song superimposed with a quieter echo of it (that is, $x(n)$ plus a delayed and attenuated copy of $x(n)$).\n\nIn this lab we address how to compensate for signal distortions. Our goal is to produce $\\hat{x}(n)$, an estimate of the original signal $x(n)$, using the measured signal $y(n)$. The process of determining $\\hat{x}(n)$ using $y(n)$ is called *signal restoration*."},{"metadata":{},"cell_type":"markdown","source":"### Problem Formulation\n\nFrom a block-diagram perspective, we can think of this *degradation* as a *system* $H$, which outputs $y(n)$, our measured signal, from input $x(n)$, the true signal. \n\n<img src=\"figs/degrade.png\" width=\"700px\"></img>\n\nOur job is to design a *restoration system*, $R$, that attempts to undo the effects of $H$ to give us $\\hat{x}(n)$, a nice approximation of $x(n)$. Using the concert hall example from above, $H$ could be the acoustics of the room, and $R$ could be a specialized filter chip in a de-echoing concert microphone you bought from your local microphone dealer."},{"metadata":{},"cell_type":"markdown","source":"### Designing $R$\n\nIn general, $H$ could be *any* system (e.g., nonlinear, time-varying, etc.), and undoing it may be intractable. To simplify things, we'll assume: \n- $H$, the degradation system,\nis DT-LTI (with frequency response $H(\\omega)$ and impulse response $h(n)$).\n- $R$, the recovery system we choose, is DT-LTI (with frequency response $R(\\omega)$ and impulse response $r(n)$).\n\nAlthough it's not obvious, in many scenarios of practical interest (e.g. acoustic echoes, system lag, image blur), this simplified model where we force ourselves to think of everything as LTI is not too far from the real-life system behaviour.\n\nSince $H$ is LTI, we can describe it's behaviour as a convolution, $y(n) = (x * h)(n)$. We want to design $R$ to *undo* this convolution, performing what's called ***deconvolution***. For this problem it's easier to work in the frequency domain, where our time-signal convolution just becomes frequency-response multiplication:\n\n$$Y(\\omega) = H(\\omega) X(\\omega),$$\n\nand\n\n$$\\hat{X}(\\omega) = R(\\omega) Y(\\omega) = R(\\omega) H(\\omega) X(\\omega),$$\n\nAlgebraically, if we pick $R(\\omega) = 1 / H(\\omega)$, this will give us $\\hat{X}(\\omega) = X(\\omega)$, from which we can take the inverse DTFT to recover the original signal. Effectively, we compute $\\hat{X}(\\omega) = Y(\\omega) / H(\\omega)$.\n\nThis algorithm is known as *Fourier deconvolution* (sometimes also called \"inverse filtering\" or \"direct deconvolution\"), since we perform the deconvolution directly based on the Fourier Transform(s) of the systems involved."},{"metadata":{},"cell_type":"markdown","source":"### Issues and Alternatives\n\nFourier deconvolution computes the multiplicative inverse of $H$'s frequency response, and uses it to characterize the inverse system $R$. \n\nThis approach has two main issues:\n1. $H$ may be zero at some frequency $\\omega_0$, in which case the division is not well-defined. Intuitively, all content at $\\omega_0$ is killed, and we're left with no information about it, so we can't invert the behavior.\n2. Even if $H$ is nonzero over all frequencies, it's often very small, and so $1 / H(\\omega)$ will be huge, and amplify noise that will be present in practical setups.\n\nDue to these issues, it's more common to use *Wiener filtering* in practice for deconvolution. Wiener filtering is a more sophisticated technique that uses statistical properties of the signals and noise involved to produce better results. However, Wiener filtering and Fourier deconvolution are similar in spirit (and we don't want to go down a rabbit hole into statistical signal processing), so we'll be using Fourier in this lab.\n\nLater in the lab, we'll explore these issues and how they can affect our signals."},{"metadata":{},"cell_type":"markdown","source":"# Q1: Echo Cancellation\n\nA classic problem in signals and systems, acoustics, and digital communications is *echo cancellation*. A sender transmits a signal to someone, and they receive it, along with a delayed, attenuated copy of it. \n\nThere are many causes of this phenomenon which you can read about in the references, including:\n- Signal back reflections due to impedance mismatches in electronic circuits (see references 1, 2).\n- Audio feedback in microphones (see reference 3).\n- Acoustic properties of the space the signal is being transmitted in. For example, if you send a signal indoors, it may go in multiple directions, with part of the signal going straight to a receiver, and the rest of it bouncing off of several walls before it arrives as a delayed and attenuated copy of the first."},{"metadata":{},"cell_type":"markdown","source":"## Modelling an echo\n\nMany different models for echoes have been considered, and you can find a more comprehensive treatment of this subject in references 4 and 5. Here, we'll consider one of the simpler models for an echo, where our communication channel (the degradation system) is an LTI system with impulse response\n\n$$h(n) = \\delta(n) + \\alpha \\delta(n - k)$$\n\nwhere $0 < \\alpha < 1$ is the attenuation of the echo and $k > 0$ is the integer delay of the echo. Assuming $0 < \\alpha < 1$ means the copy is weaker than the original, and $k>0$ that the copy shows up after the original.\n\nWe can think of this as a channel that transmits the signal perfectly and instantaneously, and also with a $k$-step delay and some attenuation along an echo path. In this problem, we'll send some audio over this channel, and try to undo the corruptions it introduces using deconvolution."},{"metadata":{},"cell_type":"markdown","source":"## A Brief Intro to Audio Signals\n\nIn this question, we'll use [\"Take on Me\" by a-ha](https://www.youtube.com/watch?v=djV11Xbc914) as our test signal. We have provided it for you as a [.wav](https://en.wikipedia.org/wiki/WAV) (pronounced \"wave\") file. Run the cell below and have a listen! It's normal if the cell takes a few seconds to load. You'll see a graphic display pop up with a play button once it's finished loading."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"ipd.Audio(\"wavs/TakeOnMe.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we'll read the file as a numpy array using scipy's WAV file API. The `read` function returns two things: the sampling rate of the audio, and the signal itself."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"# In signal processing code, \"fs\" is conventionally used for sampling frequency in Hertz (cycles/second)\nfs, data = wavfile.read(\"wavs/TakeOnMe.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Typically, digital audio is sampled at 44.1 kHz, or 44100 Hz, although some more modern formats use 48 kHz. This is motivated by the fact that the human ear can only hear up to ~20 kHz. Given this, the Nyquist criterion suggests that the sampling rate should be at least 40 kHz, so some extra wiggle room is added on. We can easily verify that we're dealing with a sampling rate of 44.1 kHz."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"print(fs) # sampling rate in Hz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When dealing with real world data, a good first step before processing it is checking what size it is using `np.shape`, just as we did when building our heart rate detector in Lab 4."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"np.shape(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perhaps surprisingly, the song, when read in as a signal, is actually a 1984500-by-2 matrix! Why?\n\nThe track's runtime is 45 seconds, and with a sampling rate of 44.1 kHz, we expect a total of \n\n$$45\\ \\text{seconds} \\cdot \\frac{44100\\ \\text{samples}}{1\\ \\text{second}} = 1984500\\ \\text{samples}$$\n\nin our data. That explains the first dimension. Why a two column matrix, though?\n\nThe reason we have two separate columns of data, rather than a single array of 1984500 samples, is due to the use of [two-channel audio](https://en.wikipedia.org/wiki/Stereophonic_sound). When you listen to music with a pair of headphones, each ear is receiving a separate audio stream, hence the need for two samples at each point in time. The same principle applies to laptops or other sound systems with two speakers. \n\n**What this means for us is that each channel (i.e., column of this matrix) should be processed as a distinct, 1D signal.**"},{"metadata":{},"cell_type":"markdown","source":"As a final step before moving on, we'll crop our song to the first 10 seconds. This will make processing go much faster, and we'll still be able to hear what's going on throughout the process."},{"metadata":{"trusted":false},"cell_type":"code","source":"n_sec = 10\ndata_cropped = data[:n_sec * fs, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a sanity check, run the cell below to play the first 10 seconds of the song. The display should show that the file has 10 seconds of audio, and it should sound exactly the same as before."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"wavfile.write(\"wavs/cropped_TakeOnMe.wav\", fs, data_cropped)\nipd.Audio(\"wavs/cropped_TakeOnMe.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's make some echoes!"},{"metadata":{},"cell_type":"markdown","source":"## Q1a: Transmission\n\nImplement the function `transmit` below to simulate the echo channel. As a reminder, so that you don't have to keep scrolling back and forth, we're modelling the channel as an LTI system with impulse response\n\n$$h(n) = \\delta(n) + \\alpha \\delta(n-k)$$\n\nwhere $\\alpha$ (the attentuation factor) and $k$ (the delay of the echo in samples) are provided to you as function arguments, along with the signal to transmit, $x$. Your function should return the result of transmitting $x$ over the channel, performing the parasitic convolution we'll later be trying to undo. \n\n**As a reminder, each audio channel should be considered as a distinct signal requiring a separate convolution when transmitting.**\n\n#### Quantization\n\nAll audio we're working with is [quantized](https://en.wikipedia.org/wiki/Quantization) to 16 bits. After processing our signal, we have to renormalize each entry to be a 16-bit integer value, or we'll introduce new distortions to it.\n\nAfter you transmit the song clip **and reassemble it into a two-channel matrix**, say `x_echoed`, apply the following line of code: \n\n**`np.int16(x_echoed / np.max(np.abs(x_echoed)) * np.iinfo(np.int16).max)`**. \n\nThis fits every value to the range [-1, +1] and then rescales it to be within the range of [-32767, 32767]. This will be the last thing you have to do before returning the result.\n\n**Hint:** All convolutions should be done in \"full\" mode to avoid cutting out data. Since we're using \"full\", there's no need to pad implicit zeros onto the echo channel impulse response; it should only be length $k+1$."},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"def transmit(x, alpha, k):\n    \"\"\"\n    Simulate transmission of a two-channel audio signal x over an LTI echo channel which sends \n    x and a copy of x delayed by k > 0 samples and attenuated by a factor 0 < alpha < 1.\n    \n    Parameters:\n    x        - The audio signal. Assumed to be an Nx2 matrix, where N is the number of audio samples.\n    alpha    - The attenuation factor. Assumed that 0 < alpha < 1.\n    k        - The delay factor, in samples. Assumed that k > 0.\n    \n    Returns:\n    x_echoed - The echoed signal.\n    \"\"\"\n    ...","execution_count":null,"outputs":[]},{"metadata":{"deletable":false,"editable":false,"trusted":false},"cell_type":"code","source":"grader.check(\"q1a\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you've finished implementing `transmit`, try it out by running the cell below, which will generate an echo at $80\\%$ the strength of the original signal and with a delay of $2 \\cdot f_s = 88200$ samples (exactly two seconds). \n\n**This means our transmitted song will be 12 seconds long.** The original copy starts at time zero and finishes 10 seconds in. The echoed copy starts 2 seconds in, and ends after 12 seconds from the start of the original copy.\n\nThis cell will take anywhere from several seconds to a minute to run depending on your laptop. Even with 10 seconds of data, we have two $10 \\cdot 44100$ entry convolutions to compute, which will take some time."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"x_echo = transmit(data_cropped, .8, 2*fs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the cell below to play your echo-corrupted song. \n\nYou should hear a second copy of the track that comes in two seconds later. This means that the first and last two seconds of the audio should only contain one track. The first two seconds will contain the start of the original, and the last two seconds will be the end of the echo. It should be easy to tell if your result is correct or not by just listening."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"wavfile.write(\"wavs/echoed_TakeOnMe.wav\", fs, x_echo)\nipd.Audio(\"wavs/echoed_TakeOnMe.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q1b: Deconvolving\n\nImplement the function `deconvolve` below using the Fourier deconvolution algorithm described in the background. Feel free to use NumPy's [fft](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html#numpy.fft.fft) and [inverse fft](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.ifft.html#numpy.fft.ifft) functions for this part.\n\nAs with Lab 3's question on oscilloscope signal alignment, we're going to encounter the issue of numerical noise here yielding an erroneously nonzero imaginary part in our final result. **Be sure to take the real part of any signal you return to avoid a fake imaginary part showing up.**"},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"def deconvolve(y, h):\n    \"\"\"\n    Perform a Fourier deconvolution to deconvolve h \"out of\" y, assuming\n    that h, y and the deconvolved signal are both real-valued.\n    \n    Parameters:\n    y - The signal to deconvolve h out of.\n    h - The impulse response used in the parasitic convolution.\n    \"\"\"\n    ...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a sanity check, let's do a toy example of echo cancellation. \n\nWe'll set:\n- $x(n) = .5 \\delta(n) + \\delta(n-1) + .5 \\delta(n-2)$, a three-point pulse.\n- $h(n) = \\delta(n) + .4 \\delta(n - 7)$, which will generate an echo with a seven-sample delay at $40\\%$ the original's strength.\n\nRun the cell below to see what this looks like."},{"metadata":{"trusted":false},"cell_type":"code","source":"x = np.array([.5, 1, .5]); h = np.array([1, 0, 0, 0, 0, 0, 0, .4]); y = np.convolve(x, h, \"full\")\n\nx_pad = np.concatenate((x, np.zeros(len(y) - len(x))))\nh_pad = np.concatenate((h, np.zeros(len(y) - len(h))))\n\nplt.figure(figsize=(16, 4))\nplt.subplot(1, 3, 1); plt.stem(x_pad); plt.title(\"Original signal x\")\nplt.subplot(1, 3, 2); plt.stem(h_pad, linefmt=\"C1\", markerfmt=\"C1o\"); plt.title(\"Impulse response h\")\nplt.subplot(1, 3, 3); plt.stem(y, linefmt=\"C2\", markerfmt=\"C2o\"); plt.title(\"Echoed version y = h * x\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to remove that second pulse that shows up in $y$. \n\nThis example is indeed \"toy\" --- the echoed pulse and the original don't temporally overlap at all, so we could just zero out the echo to solve the problem. In the real setup, however, $k$ will be small compared to the signal length and the echo and original will be superimposed. In that case, if we zeroed out the samples, we'd be cutting out data, too, and our song might sound bad or just be missing a few seconds of music altogether. Still, this is a great example to test with, since it'll be obvious if we succeed in killing the echo.\n\nRun the cell below, and see if you pass the sanity check. The plots for $x$ and $\\hat{x}$ should be identical (minus trivial differences like color)."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"x_hat = deconvolve(y, h_pad)\n\nplt.figure(figsize=(16, 4))\n\nplt.subplot(1, 3, 1); plt.stem(x_pad); plt.title(\"Original signal x\")\nplt.subplot(1, 3, 2); plt.stem(y, linefmt=\"C2\", markerfmt=\"C2o\"); plt.title(\"Echoed version y\")\nplt.subplot(1, 3, 3); plt.stem(x_hat, linefmt=\"C4\", markerfmt=\"C4o\"); plt.title(\"Recovered version x_hat\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here some more test cases for sanity checks."},{"metadata":{"deletable":false,"editable":false,"trusted":false},"cell_type":"code","source":"grader.check(\"q1b\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q1c: Echo Removal\n\nImplement `cancel_echo` below, which removes an echo of strength $\\alpha$ and sample delay $k$ from the signal `x_echo`. We want `cancel_echo(transmit(x, alpha, k), alpha, k)` to return `x` (possibly with extra zeros on the end, which are harmless) for any valid choices of $\\alpha, k$. \n\nDon't forget that:\n1. We must again renormalize the final output audio matrix to 16-bit integer values the way we did in `transmit`.\n2. The two audio channels must be treated as separate 1D signals. \n\n**Hint:** In `deconvolve`, the FFT vectors we divide must be the same length. This means that unlike in `transmit`, where you only defined the impulse response over $k+1$ indices, you should zero pad it to the same length as `x_echo` before doing the deconvolutions. An example of this is in the Q1b sanity check."},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"def cancel_echo(x_echo, alpha, k):\n    \"\"\"\n    Cancel an alpha-strength, k-sample delay echo from a two-channel audio signal x_echo\n    where k > 0 and 0 < alpha < 1.\n    \n    Parameters:\n    x_echo     - The echo-corrupted audio signal. Assumed to be an Nx2 matrix, where N is the number of audio samples.\n    alpha      - The attenuation factor. Assumed that 0 < alpha < 1.\n    k          - The delay factor, in samples. Assumed that k > 0.\n    \n    Returns:\n    x_echoless - The signal with the echo cancelled.\n    \"\"\"\n    ...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, run the cell below to see how well your echo cancellation algorithm works! If it's correct:\n- The audio file will be 12 seconds long.\n- The first 10 seconds will be the original copy of the song.\n- The last 2 seconds will be empty, as those audio samples are all zeros. Since we cancelled the echo, which was the only thing present at the end of our echoed recording, there's now no music there. Don't worry about these data-less samples, they're harmless. We could crop them to get the exact same signal if we really wanted to, but it doesn't matter as we don't hear anything."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"x_cleaned = cancel_echo(x_echo, .8, 2*fs)\nwavfile.write(\"wavs/echoless.wav\", fs, x_cleaned)\nipd.Audio(\"wavs/echoless.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you've successfully removed the echo, move on to Q1d."},{"metadata":{},"cell_type":"markdown","source":"## Q1d: Noisy Deconvolution\n\nWe removed an echo from a *clean* audio recording, which is no small feat! However, we haven't accounted for noise—we've assumed that the only unwanted effect that occurs is due to the echo itself. Fourier deconvolution's main flaw is that it tends to amplify noise, a problem which we'll now explore."},{"metadata":{},"cell_type":"markdown","source":"### Noise Model\n\nWe'll assume an *additive* noise model: after the parasitic convolution, there is a *noise signal*, which we'll denote as $z$, that is added to the final signal just before measurement. Below is a block diagram. Note that unlike $x$ and $h$, $z$ is random, although we won't dive too much into that aspect.\n\n<img src=\"figs/deconv_noise_model.png\" alt=\"drawing\" style=\"width:500px;\"/>\n\nNow, we assume $y(n) = (x * h)(n) + z(n)$, and we again want to recover $x$ given only $y, h$. To get a sense for how a noised, echoed signal differs from a noiseless one, let's add a small amount of white Gaussian noise to `x_echo`, the signal from before."},{"metadata":{"trusted":false},"cell_type":"code","source":"x_echo_noisy = x_echo + 800 * np.random.randn(x_echo.shape[0], x_echo.shape[1])\nx_echo_noisy = np.int16(x_echo_noisy / np.max(np.abs(x_echo_noisy)) * 32767)\n\nwavfile.write(\"wavs/echoed_noised_TakeOnMe.wav\", fs, x_echo_noisy)\nipd.Audio(\"wavs/echoed_noised_TakeOnMe.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we'll call the same echo cancellation function from before, and see what happens."},{"metadata":{"trusted":false},"cell_type":"code","source":"x_cleaned_noisy = cancel_echo(x_echo_noisy, .8, 2*fs)\nwavfile.write(\"wavs/echoless_noisy.wav\", fs, x_cleaned_noisy)\nipd.Audio(\"wavs/echoless_noisy.wav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- BEGIN QUESTION -->\n\n**Q:** How does the noised, echoed version sound (the one we get by running the first cell in this part) compared to the echoed version from before? You should still hear the echo, but there's also white noise that's been added. What does the white noise sound like?"},{"metadata":{},"cell_type":"markdown","source":"**Q:** You should hear that the echo was removed, just as in the noiseless case. What about the noise, though? Is it louder or softer than before the deconvolution?"},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">**A:** (TODO) </span>\n    \n<span style=\"color:blue\">**A:** (TODO) </span>"},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n### Analyzing Noisy Deconvolution\n\nJust as understanding how to deconvolve signals is most easily done in the frequency domain, so is performing an error analysis. We know that \n\n$$y(n) = (x * h)(n) + z(n) \\implies Y(\\omega) = X(\\omega)H(\\omega) + Z(\\omega).$$\n\nThe Fourier deconvolution algorithm returns $\\hat{X}(\\omega)$, an estimate of $X(\\omega)$, which is computed as \n\n$$\\hat{X}(\\omega) = \\dfrac{Y(\\omega)}{H(\\omega)} = \\dfrac{X(\\omega)H(\\omega) + Z(\\omega)}{H(\\omega)} = X(\\omega) + \\dfrac{Z(\\omega)}{H(\\omega)},$$\n\nso the difference between our estimate, $\\hat{X}$, and the true spectrum, $X$, is\n\n$$\\hat{X}(\\omega) - X(\\omega) = \\dfrac{Z(\\omega)}{H(\\omega)}.$$\n\nIn general, this will be a complex number, which isn't very useful as a way to quantify error. Instead, we can consider the *magnitude* of the error between the estimated spectrum and the true one:\n\n$$|\\hat{X}(\\omega) - X(\\omega)| = \\dfrac{|Z(\\omega)|}{|H(\\omega)|}$$."},{"metadata":{},"cell_type":"markdown","source":"As a final sanity check on your understanding of how Fourier deconvolution performs in the presence of noise, answer the following questions. \n\nAssume that for all $\\omega$, $|Z(\\omega)| = \\sigma$, which roughly says that the noise is \"equally strong\" at all frequencies. Truly, $z$ is random, so it's wrong to think of $Z$ as a DTFT in the typical sense, but we'll defer those details to EECS 225A. Our analysis below still gets at the heart of the issue and the tradeoffs involved."},{"metadata":{},"cell_type":"markdown","source":"<!-- BEGIN QUESTION -->\n\n**Q:** Suppose $\\sigma = 0$. Can we perfectly recover $X$, assuming that $|H(\\omega)| > 0\\ \\forall \\omega$? Explain."},{"metadata":{},"cell_type":"markdown","source":"**Q:** As $\\sigma$ increases, intuitively, is our noise getting stronger or weaker? Is it easier or more difficult to recover $X$?"},{"metadata":{},"cell_type":"markdown","source":"**Q:** The impulse response of our degradation system, which caused the echo, is $h(n) = \\delta(n) + \\alpha \\delta(n -k)$, and so its frequency response is $H(\\omega) = 1 + \\alpha e^{-i\\omega k}$. For concreteness, take $k=1$ and $\\alpha=.8$. \n\nWhich range of frequencies do you expect the noise amplification to be worst at: high ones, or low ones? Why? (*Hint*: Compute $|H(0)|$ and $|H(\\pi)|$)."},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">**A:** (TODO) </span>\n    \n<span style=\"color:blue\">**A:** (TODO) </span>\n    \n<span style=\"color:blue\">**A:** (TODO) </span>"},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n# Q2: *Picture* This - 2D Signal Processing!\nNearly all the techniques covered thus far in the class and this lab for 1 dimensional signals are only a few modifications away from being applied to multi-dimensional ones. In this question we'll delve into the math and mystery of operating with 2 dimensional signals called images!\n\n## Q2a: Intro to Image Processing\n\n### Images as Signals\nImages are just ordinary discrete-time signals that like to disguise themselves as rectangles, here's how:  \n\nImagine we have a discrete-time signal of duration $N$, and our desired image is a matrix of width $W$ and height $H$. We assert that $N=W\\cdot H$, so that each sample from the signal has a spot in the image matrix. We can now, by a convention of the imaging world, start building our image by taking the first $W$ elements of our 1D signal and inserting them as the top row of the image matrix, the next $W$ elements of our signal become the second row of the matrix, and so on until we've ran out of both samples and space in the matrix; the process visually looks something like this:\n\n<img src=\"figs/imagematrix.png\" alt=\"drawing\" width=\"1000px\"/>\n\nThus an image is just a matrix, and that matrix is just a folded up discrete-time signal. In the context of images, the signal is typically a measure of **luminance**, or the amount of light present at a given 2D spatial coordinate. But how do we go from a matrix of numbers to a picture of a flower?  \n\nThere are many ways of encoding images, but for this lab we'll pretend that it's still 1850 and colour photography hasn't been invented. Our images are what's called \"grayscale\" or \"uint8\" images; a single matrix with values in the 8 bit range of $2^0-1=0$ (pure black) to $2^8-1=255$ (pure white). Values between these two correspond to various shades of gray.\n\nNow let's load an image with `plt.imread` and get some information about it with `np.info`:"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"flower1 = plt.imread(\"images/flower1.tiff\")  # load the image file\nprint(np.info(flower1)) # return info on the file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look at the shape and type rows we can see that the image really is just a 256 x 256 numpy array of 8 bit unsigned integers (i.e. only non-negative values allowed), referred to by the type `uint8`. You can ignore the other data fields for now.\n\nNow, run the cell below to see an example of displaying an image."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\n\n# for plt.imshow remember to set vmin=0, vmax=255 so it doesn't try to normalize the image\nplt.imshow(flower1, cmap=\"gray\", vmin=0, vmax=255) # display image data. cmap = colour map\nplt.ylabel(\"y\")\nplt.xlabel(\"x\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using matplotlib's imshow function we are able to see that it is an image of a flower, but what about the underlying signal, what does it look like?"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"row = 128\n\nplt.figure(figsize=(16,2))\nplt.imshow(flower1[row][None,:], aspect='auto', cmap=\"gray\")\nplt.title(\"Row {0} of Flower Image\".format(row))\nplt.xlabel(\"Pixel Index along x-axis\")\nplt.show()\n\nplt.figure(figsize=(16,4))\nplt.plot(flower1[row]) # index into 2D array, return specified row\nplt.xlabel(\"Pixel Index along x-axis\")\nplt.ylabel(\"Pixel Luminance (arbitrary units)\")\nplt.title(\"Luminance of Row {0} of Flower Image\".format(row))\nplt.xlim([0, 255])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we take a look at the 128th row of the image, or equivalently the 128th chunk of the signal, we can see two peaks in luminance (brightness) centered around x=50 and x=200, as well as a trough at x=125. Looking back at the image we see that if we took such a slice through the middle of the image, we'd see two patches of bright petals and a patch of dark pistil (center of the flower) matching these peaks and trough.  \n\nFeel free to change the row variable and explore the shapes of signal chunks produced by other slices of the flower."},{"metadata":{},"cell_type":"markdown","source":"### Root Mean Square Error (RMSE)\n\nSo now we can load and understand images visually, but what we really need is a good way to compare them numerically.  \nWe'll use the RMSE, or [Root Mean Square Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation), a popular image processing similarity/error metric:\n\n$$RMSE = \\sqrt{\\frac{1}{N}\\sum_{n=0}^{N-1}(x_{1}[n]-x_{2}[n])^{2}}=\\sqrt{\\frac{1}{W\\cdot H}\\sum_{x=0}^{W-1}\\sum_{y=0}^{H-1}(f_{1}[y,x]-f_{2}[y,x])^{2}}$$\n\nWhere $y_1, y_2$ are the image signals and $N$ is their size, or alternatively $f_1,f_2$ are image matrices with heights $H$ and widths $W$. This error formula penalizes large differences in luminance very heavily (by squaring them!). To relate this back to what you've already seen, this is the 2D version of taking the energy of the (now 2D) error signal $y_1[n] - y_2[n]$, but then taking the square root of that energy and normalizing it by multiplication with $1 / \\sqrt{N}$.\n\nImplement the RMSE function below."},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"def RMSE(im1, im2):\n    \"\"\"\n    Returns the Root Mean Square Error between two images.\n    Parameters:\n    im1     - Image 1, 2D numpy array\n    im2     - Image 2, 2D numpy array\n    \"\"\"\n    assert im1.shape == im2.shape, \"shape mismatch\"\n    im1, im2 = im1.astype(np.float64), im2.astype(np.float64) # avoid overflow errors\n    ...","execution_count":null,"outputs":[]},{"metadata":{"deletable":false,"editable":false,"trusted":false},"cell_type":"code","source":"grader.check(\"q2a\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the RMSE of the flower and the lizard.\n\nAs a sanity check on your answers, consider the following questions (you don't need to answer them here, but they're worth considering):  \nWhat is the maximum and minimum RMSE we can have if our data is limited to the range 0-255? What do we expect to be the RMSE between an image and itself?"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"flower1 = plt.imread(\"images/flower1.tiff\")\nlizard = plt.imread(\"images/lizard.tiff\")\nprint(RMSE(flower1,lizard))\nprint(RMSE(flower1, flower1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we can understand and compare images we're all set to process them!"},{"metadata":{},"cell_type":"markdown","source":"## Q2b: *2D* Convolution? 1D Was Hard Enough Already!\n2D convolution is conceptually similar to 1D convolution, but there's a trick. If we slid a 1D filter over the entire image signal row by row, we would only being able to filter features along the x axis, and would carry over information from one edge of the image to the other. Thus our filter has to also be 2D, often called a \"kernel\", and flipped/slid both along the x and y axes.\n\nThe process of 2D convolution should seem familiar: you first flip the kernel along the x and y axes, and then slide it along the image matrix. The kernel will always be of an odd dimension, typically 3x3 or 5x5, and thus have exactly one center point. This center point is aligned to a pixel in your image matrix, and the output pixel is the dot product of the kernel at that position and that portion of the image matrix.  \n\nIf the kernel \"goes over the edge\" of the image matrix we can deal with it just as in the 1D case, by either pretending the image matrix is padded by zeroes (what we will typically do), or wrap the kernel around to the other side of the matrix (helps to combat some edge artifacts). The second full method is demonstrated in the helpful .gif below (run the cell to see it):"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"from IPython.display import HTML\nHTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif\">')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Interpretation of Frequencies\n\n2D kernel convolution has entire chapters of textbooks dedicated to its intricacies (A good reference text is Digital Image Processing by Gonzalez and Woods), and so the main take-away for now is that it's just sliding and dot products as in the 1D case.\n\nKernels are filters and so can be classified as low-pass, high-pass, band-pass and band-stop just like in 1D. But what exactly does a low-pass filter mean for an image? Low-pass filtering music will leave you with just the sound of the bass, but in 2D it'll leave you with a blurrier image:\n\n<img src=\"figs/frequency.png\" alt=\"drawing\" width=\"800px\" align=\"center\"/>  \n\nFor images, signal elements represent luminance, so high frequencies correspond to rapid changes in this luminance, like what you'd find in a checkerboard pattern. On the opposite end of the spectrum (pun intended), lower frequencies represent smoother visual transitions like the slow gradient of a sunset."},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Blur\n\nLet's try out what we've learned so far by convolving an image matrix with a gaussian kernel, which will take a weighted average of patches of the image, hopefully acting like a low-pass filter. The gaussian kernel is given by:\n\n$$\\text{gkern}(x,y) = \\dfrac{1}{2\\pi\\sigma^{2}}\\exp\\left\\{ \\dfrac{-(x^{2}+y^{2})}{2\\sigma^{2}}\\right\\} $$\n\nThe effect of convolving an image with this filter is often referred to as \"Gaussian blur\", due to the fact that 2D low-pass filtering is just blurring, as discussed above. If you go into image editing software such as Adobe Photoshop, you'll find commands that allow you to apply this same Gaussian blur to images - this filtering operation is what Photoshop is doing behind the scenes!"},{"metadata":{"trusted":false},"cell_type":"code","source":"def gkern(size=5, sigma=1.0):\n    \"\"\"\n    Returns a gaussian kernel with zero mean.\n    \n    Adapted from:\n    https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n    \n    Parameters:\n    size    - Sidelength of gaussian kernel\n    sigma   - Value of standard deviation\n    \"\"\"\n    ax = np.arange(-size // 2 + 1.0, size // 2 + 1.0)\n    xx, yy = np.meshgrid(ax, ax)\n    kernel = np.exp(-(xx**2 + yy**2) / (2.0 * sigma**2))\n    return kernel / np.sum(kernel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Code for producing the kernel is given above; it's up to you to use the [scipy.signal.convolve2d()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html) function to apply the filter to the **lizard.tiff** image.  \n\n**IMPORTANT: Use `mode=\"same\"` and keep `boundary` as the default value (padding the image matrix with zeroes before convolution).**\n\nWe'll experiment with different values of \"sigma\" and \"size\" and be prepared to report on their effects. Your task is to use plt.imshow() to display 4 images: the original lizard, plus three **labelled** blurrings of it. So in total, you must create and plot:\n1. The original lizard image.\n2. A version blurred with **high sigma and small kernel size**.\n3. A version blurred with **low sigma and large kernel size**.\n4. A version blurred with **high sigma and large kernel size**.\n\nDefinitions of \"high/low sigma\" and \"high/low kernel size\" have been given for you in the cell.\n\nHow you actually display the images (e.g., in a 2x2 grid using `plt.subplot`, or just by making 4 separate figures) is up to you; we don't care for grading purposes.  \n\n**Don't forget to set `cmap=\"gray\"` in your calls to `plt.imshow`**"},{"metadata":{},"cell_type":"markdown","source":"<!-- BEGIN QUESTION -->\n\n"},{"metadata":{"jupyter":{"outputs_hidden":false},"scrolled":false,"tags":[],"trusted":false},"cell_type":"code","source":"high_sigma = 5\nlow_sigma = 1\nbig_kernel = 50\nsmall_kernel = 5\n\n...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n<!-- BEGIN QUESTION -->\n\n**Q: What visual effect does increasing sigma have for blurring? What happens to the gaussian kernel (what other filter does it approach)?**\n\n**Q: What visual effect does increasing kernel size have for blurring? Now change mode=\"same\" to mode=\"full\" in all the signal.convolve2d calls in the cell above and run it again. What happens to the edges of the image (do they get brighter/darker?), why?**\n\n**Q: What are the first visual features of the lizard to be blurred away? What are the last? What does this say about the frequencies of those features?**\n"},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">A: (TODO) </span>\n\n<span style=\"color:blue\">A: (TODO) </span>\n\n<span style=\"color:blue\">A: (TODO) </span>"},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n## Q2c: Image Sharpening\n\nAnother feature in image processing software is the \"sharpening\" function. Here, we'll see how we can sharpen a picture by blurring it! If we blur an image, and subtract this blur from the original image, we end up erasing a lot of its low-frequency components and make it look comparatively sharper.\n\n1. Try sharpening **flower2.tiff**, experiment with different values of sigma, kernel size, and alpha/beta (to perform a weighted subtraction of the image and it's blur) and use plt.imshow() to display the original picture and its sharpened version."},{"metadata":{},"cell_type":"markdown","source":"You should definitely be able to see some sharpening, especially around the edges of the leaves, but the final result won't  be amazing. In Question 2c, we'll try to find a smarter way of approaching this general problem of image denoising."},{"metadata":{},"cell_type":"markdown","source":"<!-- BEGIN QUESTION -->\n\n"},{"metadata":{"jupyter":{"outputs_hidden":false},"tags":[],"trusted":false},"cell_type":"code","source":"alpha = ...\nbeta = ...\nflower2 = plt.imread(\"images/flower2.tiff\")\ngaussian_kernel = ...\nflower2_blurred = ...\nflower2_sharpened = alpha*flower2 - beta*flower2_blurred\n\n\n# Plot\nplt.figure(figsize=(20,10))\nplt.subplot(1, 3, 1)\nplt.imshow(flower2, cmap=\"gray\", vmin=0, vmax=255)\nplt.title(\"Unsharpened\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(flower2_blurred, cmap=\"gray\", vmin=0, vmax=255)\nplt.title(\"Blurred\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(flower2_sharpened, cmap=\"gray\", vmin=0, vmax=255)\nplt.title(\"Sharpened\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n## Q2D: 2D DFT\n\nFinally, before discussing 2D deconvolution, let's reintroduce one last method from our 1D signal analysis toolbox: the Discrete Fourier Transform. The 2D DFT is an advanced topic which is explored in more detail in future classes such as EECS 225B, but for now let's just figure out how to read the magnitude plot of the 2D DFT. Just like we typically do for the 1D DFT, we will use `np.fft.fftshift` to center the lowest frequency (DC) in the middle of the plot. The 2D DFT formula looks exactly like the 1D formula, but summed over two axes:\n\n$$ F(\\omega_x,\\omega_y)=\\frac{1}{M\\cdot N}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(y,x)e^{-2\\pi\\left(\\dfrac{k}{M}x+\\dfrac{l}{N}y\\right)} $$\n\nIn the 2D DFT we thus have two directions in which we care about frequencies appearing in, x and y; and so we instead of choosing an N point DFT we now choose to sample a (M,N) point 2D DFT. In return we get a 2 dimensional magnitude plot which will show us the distribution of frequencies along both axes, as displayed below (after being fft-shifted):\n\n<img src=\"figs/2d_dft.png\" alt=\"drawing\" width=\"800px\"/>\n\nAs we can see, low frequencies are now in the center of the 2D plot, and high frequencies tend towards the corners.\n\nNow a demonstration of calculating and plotting the 2D DFT of a test image:"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"test_stripes = plt.imread(\"images/test_stripe.tiff\")\nplt.figure(figsize=(8, 8))\nplt.imshow(test_stripes, cmap=\"gray\")\nplt.title(\"Horizontal Stripe\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\ntest_fft = np.fft.fft2(test_stripes) # take 2D DFT with default parameters\ntest_fft = np.fft.fftshift(test_fft) # shift low frequencies to center of plot\ntest_fft = np.abs(test_fft) # calculate magnitude of DFT\nplt.imshow(test_fft)\nplt.colorbar()\nplt.title(\"2D DFT of horizontal stripe\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot is a little hard to see, so let's zoom into the middle:"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\ntest_fft = np.fft.fft2(test_stripes) # take 2D DFT with default parameters\ntest_fft = np.fft.fftshift(test_fft) # shift low frequencies to center of plot\ntest_fft = np.abs(test_fft) # calculate magnitude of DFT\nplt.imshow(test_fft, aspect=\"auto\")\nplt.xlim(231,235)\nplt.colorbar()\nplt.title(\"2D DFT of horizontal stripe\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at this magnitude plot for the 2D DFT of horizontal stripes, we see that its energy is concentrated at x=233, the middle of the plot, and radiates up and down. Looking at our 2D DFT chart above, we see that this means that we have mostly low frequencies in the y direction, and have nothing interesting happening in the x direction of the image. This should make sense given that our image is completely static along the x axis, each column being a copy of the previous; no change = DC = zero extent in the 2D DFT.\n\nNow let's take a vertical slice of the image and the 2D DFT:"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 4))\nplt.subplot(1, 2, 1)\nplt.plot(test_stripes[:,233])\nplt.xlim([0, 511])\nplt.title(\"Column of Horizontal Stripes\")\n\nplt.subplot(1, 2, 2)\nplt.plot(test_fft[:,233])\nplt.title(\"Column of 2D DFT\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vertical slice of the image looks like a rectangle (a 'rect' signal), whose fourier transform we know is a sinc function. And that's exactly what the vertical slice of the 2D DFT appears to be!\n\nThis is not a coincidence, and exploration of this relationship between image slices and 2D DFT slices led to the discovery of the [Radon transform](https://en.wikipedia.org/wiki/Radon_transform), which in turn led to the development of medical technologies such as [Computed Tomography Scanning](https://en.wikipedia.org/wiki/CT_scan).\n\nIf we look at the 2D DFT of a 'natural' image, like flower1, you'll often only see a dot:"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.imshow(np.abs(np.fft.fftshift(np.fft.fft2(flower1))))\nplt.title(\"2D DFT log-Magnitude of flower image\")\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is because natural scenery rarely contains much high-frequency content. Apart from their edges, object luminances tend to change smoothly rather than looking like TV static. Natural images are thus almost entirely low-frequency content, meaning almost all energy of the 2D DFT would be concentrated in its center. You can read more about this phenomenon in  [**this paper**](http://web.mit.edu/torralba/www/ne3302.pdf).\n\nIt's often more interesting to look at the **log**-magnitude of a 2D DFT (essentially changing the magnitude scale to decibels, just as you saw for bode plots in 16B)."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.imshow(np.log(abs(np.fft.fftshift(np.fft.fft2(flower1))))) # take the log!\nplt.title(\"2D DFT log-Magnitude of flower image\")\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's your turn: Plot the 2D DFT **log**-magnitude of the lizard image from before and the 2D DFT magnitude of its blurriest variant (the high sigma and high kernel size one), and see if you can spot what happened to the distribution of frequencies in the image."},{"metadata":{},"cell_type":"markdown","source":"<!-- BEGIN QUESTION -->\n\n"},{"metadata":{"jupyter":{"outputs_hidden":false},"tags":[],"trusted":false},"cell_type":"code","source":"...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Q: Where in the 2D DFT magnitude plot is most of the energy concentrated for the blurred image? What frequencies (and in what directions) does this correspond to?**"},{"metadata":{},"cell_type":"markdown","source":"_Type your answer here, replacing this text._"},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n# Q3: Image Deconvolution, The 1.5 Billion Dollar Problem\n\nNow that we've gained some intuition about image matrix manipulation, convolution, and the 2D DFT, we can tackle a real-life problem which plagued scientists working on the **Hubble Space Telescope** in the 90's. The problem was that light, like most things in the universe, likes to spread out over distance. If you've ever pointed a flashlight into the night sky:  \n\n<br>\n<center> <img src=\"figs/light.jpg\" alt=\"drawing\" width=\"400px\"/></center>\n<br>\n\nYou've probably noticed that light spreads out in a cone shape rather than stay a cylindrical beam. This light spread is reasonably modeled via a Gaussian distribution, as are most things in life. The tricky bit comes down to figuring out what exact parameters — sigma and kernel size — to use. "},{"metadata":{},"cell_type":"markdown","source":"## The Hubble Space Telescope Disaster\n\nWhen scientists first received images from the Hubble Space Telescope, they were devastated to find them all catastrophically blurred! As it turned out, the main mirror of the telescope had been polished into the [wrong shape](https://en.wikipedia.org/wiki/Hubble_Space_Telescope#Flawed_mirror), meaning the light hitting the sensor was out of focus, and exhibited this same Gaussian spread as we discussed in the flashlight example.  \n\nThe engineers on the Hubble team were not about to let a 1.5 billion dollar project go to waste for one faulty mirror, and so they turned to math for the answer, holding a workshop with the world's leading experts on image and signal processing present to try and tackle the problem. They ended up producing a 150 page manuscript of [solutions](http://www.stsci.edu/files/live/sites/www/files/home/hst/documentation/_documents/RestorationofHSTImagesandSpectra.pdf) to the issue. Here we'll go through one of the most successful ones, and the one that the Hubble team actually ended up using to fix the problem: **image deconvolution**. "},{"metadata":{},"cell_type":"markdown","source":"## Q3a: 2D Fourier Deconvolution\n\nJust like the 1D example at the start of this lab, if we assume that our blur is modeled by a convolution of the original sharp image with a gaussian kernel, then for the 2D case\n\n$$\\hat{f}(x,y) = (f * g)(x,y) \\implies \\hat{F}(\\omega_x,\\omega_y) = F(\\omega_x,\\omega_y)\\cdot G(\\omega_x,\\omega_y) \\implies F(\\omega_x,\\omega_y) = \\frac{\\hat{F}(\\omega_x,\\omega_y)}{G(\\omega_x,\\omega_y)},$$\n\nwhere $f(x,y)$ is the original image, $g(x,y)$ is the gaussian kernel, and $\\hat{f}(x,y)$ is the resultant blurred image.    \n  \nLet's start by modifying the deconvolve function from earlier for 2D. Implement `deconvolve2d` below. The functions `np.fft.fft2` and `np.fft.ifft2` for handling 2D DFTs and inverse DFTs will be of use. Don't forget to handle numerical noise with `np.real` as before!   \n  \n**Hint:** Just like in 1D decovolution, make sure that the two FFT's are the same size when dividing them; you can use the size of the space images, `(512, 512)`, to set the size parameter in the np.fft functions, and they'll automatically zero-pad for you."},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"def deconvolve2d(f_hat, g):\n    \"\"\"\n    Perform a Fourier deconvolution to deconvolve g \"out of\" f_hat, assuming\n    that g, f_hat and the resultant deconvolved image are purely real.\n    \n    Parameters:\n    f_hat        - Image for deconvolution\n    g            - Convolution kernel \n    \"\"\"\n    ...","execution_count":null,"outputs":[]},{"metadata":{"deletable":false,"editable":false,"trusted":false},"cell_type":"code","source":"grader.check(\"q3a\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sanity Check:\nMake sure the code below runs without error, and that the image on the original left visually matches the deconvolved image on the right. This demonstrates the power of deconvolution if we know exactly what the gaussian kernel parameters are."},{"metadata":{"trusted":false},"cell_type":"code","source":"test_kernel = gkern(20,20)\ntest_blurred = scipy.signal.convolve2d(flower2, test_kernel, mode=\"same\", boundary='wrap')\ntest_deconvolved = deconvolve2d(test_blurred, test_kernel)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 3, 1)\nplt.imshow(flower2, cmap=\"gray\")\nplt.title(\"Original Flower Image\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(test_blurred, cmap=\"gray\")\nplt.title(\"Blurred Flower Image\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(test_deconvolved, cmap=\"gray\")\nplt.title(\"Deconvolved Flower Image\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q3b: Playing NASA Engineer\n\nIt's 1991, and you're an image processing engineer at NASA. Your team has been working tirelessly to correct the issues with the Hubble Space Telescope. A mission to install corrective optics is being planned but won't happen for another two years; in the meantime, you (and your government funding sources) don't want all of the pictures taken to go to waste. Fortunately, your colleagues have made a reconstruction of **space1_blurred.npy**, manually creating a sharp **space1.tiff** to use as a model dataset. Running the cell below will display the telescope's original captured image and the manually sharpened one."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.load(\"space1_blurred.npy\"), cmap=\"gray\")\nplt.title(\"Original Hubble Image\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(plt.imread(\"images/space1.tiff\"), cmap=\"gray\")\nplt.title(\"Manually Reconstructed Image\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scientists you work with were happy to provide you with this test data, but what they really want is an automated way to perform this type of enhancement using computer software. You've chosen to model the mirror's defects as a Gaussian blur, and you know that this is an LTI operation, and so you can undo it via deconvolution! \n\nBut how do we find the parameters for the Gaussian?"},{"metadata":{},"cell_type":"markdown","source":"## Brute Force Optimization: The Parametric Sweep\n\nIf our search space is small and our problem is hard to optimize analytically, we can use a *parametric sweep*, very common in any field of engineering. For example, in machine learning, this is often called a *grid search*. In a parametric sweep, we iterate over all possible combinations of the parameters we want to tune — here, just kernel size and sigma — and return the ones that give us the smallest error. In this case we want the lowest RMSE between the result of the deconvolution and the scientists' manual reconstruction.\n\nTo implement the parameteric sweep: \n1. Iterate through all combinations of $(\\text{Kernel Size}, \\sigma)$ for $\\text{Kernel Size} \\in \\{1, 3, 5, 7, ..., 19, 21, 23, 25\\}$ (remember, kernel size must be odd) and $\\sigma \\in [0, 10]$. Since you can't actually test every single real number in the interval of 0 to 10 for $\\sigma$, just pick 50 values from that interval that are spaced out evenly. The function `np.linspace` will be useful.\n    1. Using each pair $(\\text{Kernel Size}, \\sigma)$, apply the 2D Fourier deconvolution algorithm you implemented with `deconvolve2d` using a Gaussian kernel (`gkern`) on the blurred image.\n    3. Compute the RMSE between the deconvolved result `space1_blurred` and the manually reconstructed image `space1_ideal`. \n    4. Save the RMSE, sigma, kernel size, and the gaussian kernel itself if this is the smallest RMSE we've calculated so-far.\n2. Run the code cell below that will display the end results (in terms of both RMSE and the actual image) for you.\n\n**Hint**: The standard way to track the \"best result\" for these types of situations is to declare some variable, say, `best_RMSE` before you iterate, and set it to some ridiculously high value (you can use `np.inf`, numpy's value representing infinity, if you'd like, but anything above 1000 is more than big enough). Here, you'll also need a `best_sigma`, `best_kernel_size`, and `best_kernel`. Each time you get a better (lower) RMSE, update these variables. While it's not required, printing out each of them every time they're updated would be very instructive in seeing how this optimization technique converges to the optimal pair $(\\text{Kernel Size}, \\sigma)$.\n\n**Ignore any division by zero warnings; this is from our deconvolution function because we're dividing by very small values.**"},{"metadata":{},"cell_type":"markdown","source":"<!-- BEGIN QUESTION -->\n\n"},{"metadata":{"jupyter":{"outputs_hidden":false},"tags":[],"trusted":false},"cell_type":"code","source":"# Load images\nspace1_blurred = np.load(\"space1_blurred.npy\") # blurred image\nspace1_ideal   = plt.imread(\"images/space1.tiff\")     # manually reconstructed image\n\nbest_sigma = None\nbest_kernel_size = None\nbest_kernel = None\n\n...","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"# Display results  \nmeas_RMSE = RMSE(space1_blurred, space1_ideal)\nprint(\"Original RMSE: \" + str(round(meas_RMSE, 3)))\nprint(\"Best RMSE:     \" + str(round(best_RMSE, 3)))\nratio = meas_RMSE / best_RMSE\nprint(\"Numerically optimized Fourier deconvolution is {0} better in terms of RMSE.\".format(round(ratio, 3)))\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 3, 1)\nplt.imshow(space1_blurred, cmap=\"gray\")\nplt.title(\"Original Space Hubble Image\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(deconvolve2d(space1_blurred, best_kernel), cmap=\"gray\")\nplt.title(\"Deconvolved Space Hubble Image\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(space1_ideal, cmap=\"gray\")\nplt.title(\"Manually Reconstructed Hubble Image\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- END QUESTION -->\n\nResults looking good? Nice!\n\nNow, since we said that the corruption with our space telescope is consistently a Gaussian blur, and consistently the same kernel (since it's ultimately the telescope itself rather than some other random phenomenon), applying this same deconvolution method with the same kernel to any image it captures should give us the same kind of improvement! \n\nLet's use our calculated best gaussian kernel, `best_kernel`, to attempt to deconvolve another image of space that was captured, **space2_blurred.npy**, without having to manually reconstruct it.\n\nTo complete the lab, run the cell below (no code on your part required), which will apply the same deconvolution technique to this newly acquired image and display the result. After running it, you're all done."},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":false},"cell_type":"code","source":"space2_blurred = np.load(\"space2_blurred.npy\")\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.imshow(space2_blurred, cmap=\"gray\")\nplt.title(\"Blurred Hubble Image #2\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(deconvolve2d(space2_blurred, best_kernel), cmap=\"gray\")\nplt.title(\"Deconvolved Hubble Image #2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A parting note on the instability of 2D deconvolution:** 2D deconvolution is such an [ill-conditioned](https://en.wikipedia.org/wiki/Condition_number) problem that we had to give you numpy arrays for the blurred space images. This is because if we rounded the floating point data to its nearest integer representation, and saved it as a .tiff image file, that change would alone be enough that our deconvolution algorithm would never converge on an optimal solution (the error would always be massive, even for the \"correct\" gaussian kernel parameters)."},{"metadata":{},"cell_type":"markdown","source":"# References\n\n[1] *Signal reflection (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Signal_reflection)  \n[2] *AT&T Archives: Similarities of Wave Behavior*. [Link](https://www.youtube.com/watch?v=DovunOxlY1k)  \n[3] *Audio feedback (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Audio_feedback)  \n[4] *Dereverberation (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Dereverberation)  \n[5] *Stereophonic Acoustic Echo Cancellation: Theory and Implementation*. [Link](http://lup.lub.lu.se/search/ws/files/4596819/1001945.pdf)  \n[6] *Restoration of Hubble Space Telescope Images and Spectra*. [Link](http://www.stsci.edu/hst/HST_overview/documents/RestorationofHSTImagesandSpectra.pdf)  \n[7] *Richardson-Lucy deconvolution (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution)  \n[8] *Wiener deconvolution (Wikipedia)*. [Link](https://en.wikipedia.org/wiki/Wiener_deconvolution)  \n[9] *Signals, Systems, and Inference, Chapter 11: Wiener Filtering*. [Link](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-011-introduction-to-communication-control-and-signal-processing-spring-2010/readings/MIT6_011S10_chap11.pdf)  \n[10] 2D convolution GIF. [Link](https://upload.wikimedia.org/wikipedia/commons/4/4f/3D_Convolution_Animation.gif)  \n[11] *LTI Models and Convolution, Section 11.2.3: Deconvolution*. [Link](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/readings/MIT6_02F12_chap11.pdf)  \n[12] *The Scientist and Engineer's Guide to Digital Signal Processing: Chapter 17, Custom Filters and Deconvolution*. [Link](https://www.dspguide.com/ch17/2.htm)  \n[13] *Statistics of natural image categories*. [Link](http://web.mit.edu/torralba/www/ne3302.pdf)"},{"metadata":{"deletable":false,"editable":false},"cell_type":"markdown","source":"## Submission\n\nMake sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n\nPlease upload the zip file produced by the result of this command to Gradescope."},{"metadata":{"deletable":false,"editable":false,"trusted":false},"cell_type":"code","source":"grader.export(force_save=True, run_tests=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "}],"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.9.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"otter":{"OK_FORMAT":true,"tests":{"q1a":{"name":"q1a","points":5,"suites":[{"cases":[{"code":">>> assert transmit(np.ones((10, 2)), 0.1, 5).dtype == np.int16\n","hidden":false,"locked":false},{"code":">>> correct_result = np.array([[29788, 29788],\n...        [29788, 29788],\n...        [29788, 29788],\n...        [29788, 29788],\n...        [29788, 29788],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [ 2978,  2978],\n...        [ 2978,  2978],\n...        [ 2978,  2978],\n...        [ 2978,  2978],\n...        [ 2978,  2978]], dtype=np.int16)\n>>> assert np.allclose(transmit(np.ones((10, 2)), 0.1, 5), correct_result)\n","hidden":false,"locked":false},{"code":">>> correct_result = np.array([[29788, 29788],\n...        [29788, 29788],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [32767, 32767],\n...        [ 2978,  2978],\n...        [ 2978,  2978]], dtype=np.int16)\n>>> assert np.allclose(transmit(np.ones((10, 2)), 0.1, 2), correct_result)\n","hidden":false,"locked":false},{"code":">>> x = np.int16(np.sin(2 * np.pi * 100 * np.linspace(0, 100, 40)) * 32767).reshape((-1, 2))\n>>> correct_result = np.array([[     0,   3528],\n...        [ -5964,   6553],\n...        [ -5113,  19731],\n...        [-28240,  28006],\n...        [-19099,   4279],\n...        [ 11864, -24335],\n...        [ 29272, -25145],\n...        [ 13232,   2777],\n...        [-17926,  27526],\n...        [-28603,  20823],\n...        [ -6596,  -9672],\n...        [ 22946, -29116],\n...        [ 26271, -15291],\n...        [  -422,  16005],\n...        [-26633,  29014],\n...        [-22413,   8870],\n...        [  7416, -21408],\n...        [ 28771, -27226],\n...        [ 17251,  -1934],\n...        [-13980,  25567],\n...        [-32767,  29820],\n...        [-17640,      0]], dtype=np.int16)\n>>> assert np.allclose(transmit(x, 5, 2), correct_result)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q1b":{"name":"q1b","points":5,"suites":[{"cases":[{"code":">>> x = np.zeros(10); x[::2] = 1\n>>> h = np.array([1, 0, 0, 0, 0, 0, 0, .4])\n>>> y = np.convolve(x, h, mode='full')\n>>> \n>>> x_pad = np.concatenate((x, np.zeros(len(y) - len(x))))\n>>> h_pad = np.concatenate((h, np.zeros(len(y) - len(h))))\n>>> \n>>> assert np.allclose(deconvolve(y, h_pad), x_pad)\n","hidden":false,"locked":false},{"code":">>> x = np.zeros(10); x[::3] = 1\n>>> h = np.array([1, 0, 0.4,])\n>>> y = np.convolve(x, h, mode='full')\n>>> \n>>> x_pad = np.concatenate((x, np.zeros(len(y) - len(x))))\n>>> h_pad = np.concatenate((h, np.zeros(len(y) - len(h))))\n>>> \n>>> assert np.allclose(deconvolve(y, h_pad), x_pad)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q1c":{"name":"q1c","points":20,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q2a":{"name":"q2a","points":10,"suites":[{"cases":[{"code":">>> assert np.allclose(RMSE(np.ones((100, 100)), np.ones((100, 100))), 0)\n","hidden":false,"locked":false},{"code":">>> assert np.allclose(RMSE(np.ones((100, 100)), np.zeros((100, 100))), 1)\n","hidden":false,"locked":false},{"code":">>> x = np.array([[ 0.16291483, -1.12515383,  1.11386193,  0.31782348,  0.79116572,\n...          1.08393646, -0.18712036, -0.77155655, -1.268545  ,  1.30995988],\n...        [ 0.7715495 , -1.10811257,  0.10127387, -0.2621533 ,  0.27804201,\n...         -0.19440545,  1.07374157, -0.49539658,  2.24245888,  1.0427681 ],\n...        [ 0.82897543, -2.92451947,  0.26103135, -1.09298635,  0.24589367,\n...         -0.309184  ,  0.70403579, -0.04996904, -0.21615233, -0.49825486],\n...        [ 1.79702449, -0.68466898,  1.6220382 ,  0.38797885,  0.95184074,\n...         -0.61901578, -1.77087926,  1.78906078, -0.18056051,  0.09191897],\n...        [ 0.36454998,  1.49590352,  1.34517194,  0.48584432, -0.15062206,\n...         -0.51919533,  1.25395074,  0.55401978,  0.25828072,  0.4742485 ],\n...        [-0.09341236,  1.77180262, -1.67383196,  1.32261772,  0.86611657,\n...         -0.42142438, -0.94005873,  0.65777576,  1.05334734, -0.30720122],\n...        [-0.39790237, -2.18881071,  1.65045409,  0.37682502, -0.05758727,\n...          0.9052158 ,  0.09768797, -0.06351562,  0.80565105, -0.04446304],\n...        [-0.39010564,  0.94784561, -1.67498332, -1.82736957,  1.31098478,\n...          0.74464037,  0.84592751, -0.67492764, -0.00606451,  0.14267234],\n...        [-1.91583471,  0.94305587,  0.80301847, -0.70708025, -0.36161864,\n...         -0.04057423, -1.57742985, -0.06108896, -0.89314412,  0.33128844],\n...        [ 2.26859137, -0.27557173, -1.56651714,  0.11597074, -0.99676352,\n...          1.26978986, -0.28153395,  1.38577189,  0.01539422, -0.05508632]])\n>>> \n>>> y = np.array([[ 1.01466930e+00,  7.87789547e-01, -1.26062481e+00,\n...          5.57416029e-01,  1.75766356e-03,  1.51981644e-01,\n...         -5.95717465e-01,  2.84186530e+00, -1.16091644e+00,\n...          3.48534045e-01],\n...        [-1.84551083e+00, -3.38458138e+00,  1.80063659e+00,\n...         -1.19014173e+00,  8.14740629e-01, -1.39451682e+00,\n...          1.11340402e+00,  3.22442313e-01,  1.14694066e+00,\n...          4.40357153e-01],\n...        [ 2.20579416e-01,  1.88491602e-01,  5.22524304e-01,\n...         -4.23980481e-01, -1.36784487e-01,  7.01081110e-03,\n...          1.60810229e-01,  1.49585346e+00,  6.06250332e-01,\n...         -4.05520846e-01],\n...        [ 5.21109690e-01,  1.24581858e+00,  3.86993837e-01,\n...         -9.21825483e-02,  1.19803952e+00, -7.63256515e-01,\n...         -2.67627857e-01, -2.18236414e-01, -1.68762997e+00,\n...          6.17669993e-01],\n...        [-1.81733885e-02, -8.57171770e-01, -1.49820796e+00,\n...         -6.18108200e-01,  2.41270134e-01, -5.29698445e-01,\n...          1.19698384e+00, -1.69004807e+00,  6.21940539e-01,\n...          5.45081534e-01],\n...        [ 8.88320610e-01, -1.01985952e+00, -2.14754550e+00,\n...          9.57464730e-01, -1.12471734e+00,  3.70265486e-02,\n...         -2.47751934e-01, -5.60745531e-01, -4.15260240e-02,\n...         -1.12829224e+00],\n...        [-1.69500299e+00,  8.72192392e-01,  8.61850671e-01,\n...          2.22019877e-01,  5.93208436e-01,  8.05204074e-01,\n...          5.05806089e-01,  8.26395145e-01,  1.29605519e+00,\n...         -8.68742582e-01],\n...        [-1.08490683e+00, -4.47459519e-01, -7.50627330e-01,\n...         -9.43503033e-01, -2.75176341e-02, -7.30300713e-01,\n...          3.82079051e-01, -1.14662367e+00,  4.22047100e-02,\n...         -3.75573919e-01],\n...        [ 1.41427554e-01, -5.25045309e-01,  1.12340027e-01,\n...         -1.78837985e+00,  1.03767516e+00,  8.87584711e-01,\n...         -1.22066710e+00,  6.26286128e-01, -1.64446161e+00,\n...          1.32409418e+00],\n...        [-5.79779834e-01,  6.97698860e-01, -2.09736061e+00,\n...          2.20363149e+00,  3.46910409e-01,  1.62679879e+00,\n...         -1.17861077e+00, -5.28273620e-01, -4.94800255e-01,\n...         -7.69418704e-01]])\n>>> \n>>> assert np.allclose(RMSE(x, y), 1.302938192815072)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q3a":{"name":"q3a","points":10,"suites":[{"cases":[{"code":">>> x = np.sin(2 * np.pi / 512 * np.arange(512 * 512)).reshape((512, 512))\n>>> blur_kern = gkern(20, 20)\n>>> assert np.allclose(deconvolve2d(x, blur_kern), x, rtol=0.2, atol=0.2)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]}}}},"nbformat":4,"nbformat_minor":4}